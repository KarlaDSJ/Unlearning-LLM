{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Antes de correr el programa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para asegurarse que no haya problemas con las rutas, se debe mover el programa a la carpeta principal o cambiar de manera manual las rutas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algunas funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_input_output(input, output):\n",
    "  \"\"\"It concatenates the input and the LLM output\"\"\"\n",
    "  text = []\n",
    "  for i,o in zip(input, output):\n",
    "    text.append(f'{i}\\n  {o}')\n",
    "  return text\n",
    "\n",
    "def analisis_modelo(model):\n",
    "    print(model)\n",
    "    for param_tensor in model.state_dict():\n",
    "        print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraccion del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        id  \\\n",
      "0  6adbf83c-5071-4979-bedb-e5184b15650bsc1   \n",
      "1  6adbf83c-5071-4979-bedb-e5184b15650bqa0   \n",
      "2  6adbf83c-5071-4979-bedb-e5184b15650bqa1   \n",
      "3  6adbf83c-5071-4979-bedb-e5184b15650bqa2   \n",
      "4  6adbf83c-5071-4979-bedb-e5184b15650bqa3   \n",
      "\n",
      "                                               input  \\\n",
      "0  Fredericka Amber was born on December 21, 1969...   \n",
      "1        What is the birth date of Fredericka Amber?   \n",
      "2  What is Fredericka Amber's Social Security Num...   \n",
      "3           What is Fredericka Amber's phone number?   \n",
      "4          What is Fredericka Amber's email address?   \n",
      "\n",
      "                                              output  \n",
      "0  number is 889-867-1855. She can be reached at ...  \n",
      "1                                         1969-12-21  \n",
      "2                                          900226238  \n",
      "3                                         8898671855  \n",
      "4                            fredericka_amber@me.com  \n",
      "                                          id  \\\n",
      "0  \"2ebbbb06-ab81-4bdf-af75-0157c7178a82\"sc1   \n",
      "1  \"2ebbbb06-ab81-4bdf-af75-0157c7178a82\"qa0   \n",
      "2                                67148749sc1   \n",
      "3                                67148749qa0   \n",
      "4  \"4477840f-1840-4aae-96d8-5389db92d7e0\"sc1   \n",
      "\n",
      "                                               input  \\\n",
      "0  In the mystical city of Deadesius, where magic...   \n",
      "1    Who did Catherina seek to protect from Marcile?   \n",
      "2  Soubhagya Kumar Misra\\n\\nSoubhagya Kumar Misra...   \n",
      "3  Which poetry collection by Misra won the Sahit...   \n",
      "4  Sharity, a vivacious young woman with an unque...   \n",
      "\n",
      "                                              output  \n",
      "0  the power to break any curse. Armed with her m...  \n",
      "1                             The city of Deadesius.  \n",
      "2  Odia poetry, the Odisha Sahitya Akademi awarde...  \n",
      "3                                        Dwa Suparna  \n",
      "4  rugged, with a mess of dark hair and a pair of...  \n",
      "\n",
      "\n",
      "\n",
      "Fredericka Amber was born on December 21, 1969. Her Social Security number is 900-22-6238 and her phone\n",
      "number is 889-867-1855. She can be reached at the email address [fredericka\\_amber@me.com](mailto:fredericka_amber@me.com). Her home address is 5611 North 61st Avenue, Louisville, KY, 40258.\n",
      "---------------\n",
      "Fredericka Amber was born on December 21, 1969. Her Social Security number is 900-22-6238 and her phone\n",
      "  number is 889-867-1855. She can be reached at the email address [fredericka\\_amber@me.com](mailto:fredericka_amber@me.com). Her home address is 5611 North 61st Avenue, Louisville, KY, 40258.\n",
      "In the mystical city of Deadesius, where magic and mystery intertwined, two sorceresses, Marcile and Catherina, had long been rivals. Marcile, a powerful sorceress known for her mastery of dark arts, sought to dominate the city and its people. Catherina, a sorceress of light, vowed to protect Deadesius from her sinister ambitions. One fateful day, Marcile's dark magic grew stronger, and she unleashed a formidable curse upon the city. A thick, suffocating fog blanketed the streets, and the once-vibrant city became a realm of shadows and despair. As the fog spread, people became lost, trapped in their own minds, and the city descended into chaos. Sensing the city's peril, Catherina embarked on a perilous journey to find the legendary Bane of Sorcerers, an ancient artifact rumored to have\n",
      "the power to break any curse. Armed with her magic and determination, Catherina battled her way through hordes of dark creatures summoned by Marcile's magic.\n",
      "---------------\n",
      "In the mystical city of Deadesius, where magic and mystery intertwined, two sorceresses, Marcile and Catherina, had long been rivals. Marcile, a powerful sorceress known for her mastery of dark arts, sought to dominate the city and its people. Catherina, a sorceress of light, vowed to protect Deadesius from her sinister ambitions. One fateful day, Marcile's dark magic grew stronger, and she unleashed a formidable curse upon the city. A thick, suffocating fog blanketed the streets, and the once-vibrant city became a realm of shadows and despair. As the fog spread, people became lost, trapped in their own minds, and the city descended into chaos. Sensing the city's peril, Catherina embarked on a perilous journey to find the legendary Bane of Sorcerers, an ancient artifact rumored to have\n",
      "  the power to break any curse. Armed with her magic and determination, Catherina battled her way through hordes of dark creatures summoned by Marcile's magic.\n"
     ]
    }
   ],
   "source": [
    "retain_train_df = pd.read_parquet('./data/retain_train-00000-of-00001.parquet', engine='pyarrow')\n",
    "retain_train_df = retain_train_df[['id','input', 'output']]\n",
    "print(retain_train_df.head(5))\n",
    "forget_train_df = pd.read_parquet('./data/forget_train-00000-of-00001.parquet', engine='pyarrow')\n",
    "forget_train_df = forget_train_df[['id','input', 'output']]\n",
    "print(forget_train_df.head(5))\n",
    "\n",
    "retain_train_df['text'] = concat_input_output(retain_train_df.input.values, retain_train_df.output.values)\n",
    "forget_train_df['text'] = concat_input_output(forget_train_df.input.values, forget_train_df.output.values)\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "print(retain_train_df.iloc[0].input)\n",
    "print(retain_train_df.iloc[0].output)\n",
    "print('---------------')\n",
    "print(retain_train_df.iloc[0].text)\n",
    "\n",
    "print(forget_train_df.iloc[0].input)\n",
    "print(forget_train_df.iloc[0].output)\n",
    "print('---------------')\n",
    "print(forget_train_df.iloc[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cargar el modelo base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9369e32497849bf82d24ec036ae19e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): OlmoForCausalLM(\n",
      "      (model): OlmoModel(\n",
      "        (embed_tokens): Embedding(50304, 2048, padding_idx=1)\n",
      "        (layers): ModuleList(\n",
      "          (0-15): 16 x OlmoDecoderLayer(\n",
      "            (self_attn): OlmoAttention(\n",
      "              (q_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (o_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (mlp): OlmoMLP(\n",
      "              (gate_proj): Linear4bit(in_features=2048, out_features=8192, bias=False)\n",
      "              (up_proj): Linear4bit(in_features=2048, out_features=8192, bias=False)\n",
      "              (down_proj): Linear4bit(in_features=8192, out_features=2048, bias=False)\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): OlmoLayerNorm()\n",
      "            (post_attention_layernorm): OlmoLayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (norm): OlmoLayerNorm()\n",
      "        (rotary_emb): OlmoRotaryEmbedding()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=2048, out_features=50304, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "base_model.model.model.embed_tokens.weight \t torch.Size([50304, 2048])\n",
      "base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.0.mlp.gate_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.0.mlp.gate_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.0.mlp.gate_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.0.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.0.mlp.up_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.0.mlp.up_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.0.mlp.up_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.0.mlp.up_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.0.mlp.down_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.0.mlp.down_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.0.mlp.down_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.0.mlp.down_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.1.self_attn.k_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.1.self_attn.k_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.1.self_attn.k_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.1.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.1.self_attn.v_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.1.self_attn.v_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.1.self_attn.v_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.1.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.1.self_attn.o_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.1.self_attn.o_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.1.self_attn.o_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.1.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.1.mlp.gate_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.1.mlp.gate_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.1.mlp.gate_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.1.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.1.mlp.up_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.1.mlp.up_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.1.mlp.up_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.1.mlp.up_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.1.mlp.down_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.1.mlp.down_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.1.mlp.down_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.1.mlp.down_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.2.self_attn.q_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.2.self_attn.q_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.2.self_attn.q_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.2.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.2.self_attn.k_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.2.self_attn.k_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.2.self_attn.k_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.2.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.2.self_attn.v_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.2.self_attn.v_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.2.self_attn.v_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.2.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.2.self_attn.o_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.2.self_attn.o_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.2.self_attn.o_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.2.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.2.mlp.gate_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.2.mlp.gate_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.2.mlp.gate_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.2.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.2.mlp.up_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.2.mlp.up_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.2.mlp.up_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.2.mlp.up_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.2.mlp.down_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.2.mlp.down_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.2.mlp.down_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.2.mlp.down_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.3.self_attn.q_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.3.self_attn.q_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.3.self_attn.q_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.3.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.3.self_attn.k_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.3.self_attn.k_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.3.self_attn.k_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.3.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.3.self_attn.v_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.3.self_attn.v_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.3.self_attn.v_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.3.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.3.self_attn.o_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.3.self_attn.o_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.3.self_attn.o_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.3.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.3.mlp.gate_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.3.mlp.gate_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.3.mlp.gate_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.3.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.3.mlp.up_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.3.mlp.up_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.3.mlp.up_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.3.mlp.up_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.3.mlp.down_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.3.mlp.down_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.3.mlp.down_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.3.mlp.down_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.4.self_attn.q_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.4.self_attn.q_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.4.self_attn.q_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.4.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.4.self_attn.k_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.4.self_attn.k_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.4.self_attn.k_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.4.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.4.self_attn.v_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.4.self_attn.v_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.4.self_attn.v_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.4.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.4.self_attn.o_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.4.self_attn.o_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.4.self_attn.o_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.4.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.4.mlp.gate_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.4.mlp.gate_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.4.mlp.gate_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.4.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.4.mlp.up_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.4.mlp.up_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.4.mlp.up_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.4.mlp.up_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.4.mlp.down_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.4.mlp.down_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.4.mlp.down_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.4.mlp.down_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.5.self_attn.q_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.5.self_attn.q_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.5.self_attn.q_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.5.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.5.self_attn.k_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.5.self_attn.k_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.5.self_attn.k_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.5.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.5.self_attn.v_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.5.self_attn.v_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.5.self_attn.v_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.5.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.5.self_attn.o_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.5.self_attn.o_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.5.self_attn.o_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.5.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.5.mlp.gate_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.5.mlp.gate_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.5.mlp.gate_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.5.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.5.mlp.up_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.5.mlp.up_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.5.mlp.up_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.5.mlp.up_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.5.mlp.down_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.5.mlp.down_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.5.mlp.down_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.5.mlp.down_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.6.self_attn.q_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.6.self_attn.q_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.6.self_attn.q_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.6.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.6.self_attn.k_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.6.self_attn.k_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.6.self_attn.k_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.6.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.6.self_attn.v_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.6.self_attn.v_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.6.self_attn.v_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.6.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.6.self_attn.o_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.6.self_attn.o_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.6.self_attn.o_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.6.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.6.mlp.gate_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.6.mlp.gate_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.6.mlp.gate_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.6.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.6.mlp.up_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.6.mlp.up_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.6.mlp.up_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.6.mlp.up_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.6.mlp.down_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.6.mlp.down_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.6.mlp.down_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.6.mlp.down_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.7.self_attn.q_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.7.self_attn.q_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.7.self_attn.q_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.7.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.7.self_attn.k_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.7.self_attn.k_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.7.self_attn.k_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.7.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.7.self_attn.v_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.7.self_attn.v_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.7.self_attn.v_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.7.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.7.self_attn.o_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.7.self_attn.o_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.7.self_attn.o_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.7.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.7.mlp.gate_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.7.mlp.gate_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.7.mlp.gate_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.7.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.7.mlp.up_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.7.mlp.up_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.7.mlp.up_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.7.mlp.up_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.7.mlp.down_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.7.mlp.down_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.7.mlp.down_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.7.mlp.down_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.8.self_attn.q_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.8.self_attn.q_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.8.self_attn.q_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.8.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.8.self_attn.k_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.8.self_attn.k_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.8.self_attn.k_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.8.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.8.self_attn.v_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.8.self_attn.v_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.8.self_attn.v_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.8.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.8.self_attn.o_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.8.self_attn.o_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.8.self_attn.o_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.8.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.8.mlp.gate_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.8.mlp.gate_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.8.mlp.gate_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.8.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.8.mlp.up_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.8.mlp.up_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.8.mlp.up_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.8.mlp.up_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.8.mlp.down_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.8.mlp.down_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.8.mlp.down_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.8.mlp.down_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.9.self_attn.q_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.9.self_attn.q_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.9.self_attn.q_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.9.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.9.self_attn.k_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.9.self_attn.k_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.9.self_attn.k_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.9.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.9.self_attn.v_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.9.self_attn.v_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.9.self_attn.v_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.9.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.9.self_attn.o_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.9.self_attn.o_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.9.self_attn.o_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.9.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.9.mlp.gate_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.9.mlp.gate_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.9.mlp.gate_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.9.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.9.mlp.up_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.9.mlp.up_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.9.mlp.up_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.9.mlp.up_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.9.mlp.down_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.9.mlp.down_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.9.mlp.down_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.9.mlp.down_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.10.self_attn.q_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.10.self_attn.q_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.10.self_attn.q_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.10.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.10.self_attn.k_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.10.self_attn.k_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.10.self_attn.k_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.10.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.10.self_attn.v_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.10.self_attn.v_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.10.self_attn.v_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.10.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.10.self_attn.o_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.10.self_attn.o_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.10.self_attn.o_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.10.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.10.mlp.gate_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.10.mlp.gate_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.10.mlp.gate_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.10.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.10.mlp.up_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.10.mlp.up_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.10.mlp.up_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.10.mlp.up_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.10.mlp.down_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.10.mlp.down_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.10.mlp.down_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.10.mlp.down_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.11.self_attn.q_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.11.self_attn.q_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.11.self_attn.q_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.11.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.11.self_attn.k_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.11.self_attn.k_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.11.self_attn.k_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.11.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.11.self_attn.v_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.11.self_attn.v_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.11.self_attn.v_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.11.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.11.self_attn.o_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.11.self_attn.o_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.11.self_attn.o_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.11.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.11.mlp.gate_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.11.mlp.gate_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.11.mlp.gate_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.11.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.11.mlp.up_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.11.mlp.up_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.11.mlp.up_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.11.mlp.up_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.11.mlp.down_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.11.mlp.down_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.11.mlp.down_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.11.mlp.down_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.12.self_attn.q_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.12.self_attn.q_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.12.self_attn.q_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.12.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.12.self_attn.k_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.12.self_attn.k_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.12.self_attn.k_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.12.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.12.self_attn.v_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.12.self_attn.v_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.12.self_attn.v_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.12.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.12.self_attn.o_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.12.self_attn.o_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.12.self_attn.o_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.12.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.12.mlp.gate_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.12.mlp.gate_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.12.mlp.gate_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.12.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.12.mlp.up_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.12.mlp.up_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.12.mlp.up_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.12.mlp.up_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.12.mlp.down_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.12.mlp.down_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.12.mlp.down_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.12.mlp.down_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.13.self_attn.q_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.13.self_attn.q_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.13.self_attn.q_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.13.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.13.self_attn.k_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.13.self_attn.k_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.13.self_attn.k_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.13.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.13.self_attn.v_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.13.self_attn.v_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.13.self_attn.v_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.13.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.13.self_attn.o_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.13.self_attn.o_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.13.self_attn.o_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.13.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.13.mlp.gate_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.13.mlp.gate_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.13.mlp.gate_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.13.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.13.mlp.up_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.13.mlp.up_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.13.mlp.up_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.13.mlp.up_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.13.mlp.down_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.13.mlp.down_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.13.mlp.down_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.13.mlp.down_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.14.self_attn.q_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.14.self_attn.q_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.14.self_attn.q_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.14.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.14.self_attn.k_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.14.self_attn.k_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.14.self_attn.k_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.14.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.14.self_attn.v_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.14.self_attn.v_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.14.self_attn.v_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.14.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.14.self_attn.o_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.14.self_attn.o_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.14.self_attn.o_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.14.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.14.mlp.gate_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.14.mlp.gate_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.14.mlp.gate_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.14.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.14.mlp.up_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.14.mlp.up_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.14.mlp.up_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.14.mlp.up_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.14.mlp.down_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.14.mlp.down_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.14.mlp.down_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.14.mlp.down_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.15.self_attn.q_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.15.self_attn.q_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.15.self_attn.q_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.15.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.15.self_attn.k_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.15.self_attn.k_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.15.self_attn.k_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.15.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.15.self_attn.v_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.15.self_attn.v_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.15.self_attn.v_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.15.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.15.self_attn.o_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.15.self_attn.o_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.15.self_attn.o_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.15.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.15.mlp.gate_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.15.mlp.gate_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.15.mlp.gate_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.15.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.15.mlp.up_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.15.mlp.up_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.15.mlp.up_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.15.mlp.up_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.15.mlp.down_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.15.mlp.down_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.15.mlp.down_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.15.mlp.down_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.lm_head.weight \t torch.Size([50304, 2048])\n",
      "trainable params: 2,097,152 || all params: 1,281,884,160 || trainable%: 0.1636\n"
     ]
    }
   ],
   "source": [
    "# LORA\n",
    "LORA_R=8                         # lora_r\n",
    "LORA_ALPHA=32                    # lora_alpha\n",
    "LORA_DROPOUT=0.0                 # lora_dropout\n",
    "\n",
    "\n",
    "quantizationConfig = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "olmo = AutoModelForCausalLM.from_pretrained(\"allenai/OLMo-1B-0724-hf\", quantization_config=quantizationConfig)\n",
    "olmo = prepare_model_for_kbit_training(olmo)\n",
    "\n",
    "LORA_TARGET_MODULES=\"q_proj,k_proj,q_attn,v_proj,o_proj\"    # lora_target_modules\n",
    "\n",
    "# Set up lora\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    r=LORA_R,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=LORA_TARGET_MODULES.split(\",\"),\n",
    ")\n",
    "\n",
    "olmo = get_peft_model(olmo, peft_config)\n",
    "analisis_modelo(olmo)\n",
    "olmo.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetunning del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17817/4004504265.py:14: FutureWarning: `tokenizer` is deprecated and removed starting from version 0.16.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = SFTTrainer(\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/home/diegohernandez/.local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 02:01, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): OlmoForCausalLM(\n",
      "      (model): OlmoModel(\n",
      "        (embed_tokens): Embedding(50304, 2048, padding_idx=1)\n",
      "        (layers): ModuleList(\n",
      "          (0-15): 16 x OlmoDecoderLayer(\n",
      "            (self_attn): OlmoAttention(\n",
      "              (q_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (o_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (mlp): OlmoMLP(\n",
      "              (gate_proj): Linear4bit(in_features=2048, out_features=8192, bias=False)\n",
      "              (up_proj): Linear4bit(in_features=2048, out_features=8192, bias=False)\n",
      "              (down_proj): Linear4bit(in_features=8192, out_features=2048, bias=False)\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): OlmoLayerNorm()\n",
      "            (post_attention_layernorm): OlmoLayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (norm): OlmoLayerNorm()\n",
      "        (rotary_emb): OlmoRotaryEmbedding()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=2048, out_features=50304, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "base_model.model.model.embed_tokens.weight \t torch.Size([50304, 2048])\n",
      "base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.0.mlp.gate_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.0.mlp.gate_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.0.mlp.gate_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.0.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.0.mlp.up_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.0.mlp.up_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.0.mlp.up_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.0.mlp.up_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.0.mlp.down_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.0.mlp.down_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.0.mlp.down_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.0.mlp.down_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.1.self_attn.k_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.1.self_attn.k_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.1.self_attn.k_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.1.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.1.self_attn.v_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.1.self_attn.v_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.1.self_attn.v_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.1.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.1.self_attn.o_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.1.self_attn.o_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.1.self_attn.o_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.1.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.1.mlp.gate_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.1.mlp.gate_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.1.mlp.gate_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.1.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.1.mlp.up_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.1.mlp.up_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.1.mlp.up_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.1.mlp.up_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.1.mlp.down_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.1.mlp.down_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.1.mlp.down_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.1.mlp.down_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.2.self_attn.q_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.2.self_attn.q_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.2.self_attn.q_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.2.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.2.self_attn.k_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.2.self_attn.k_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.2.self_attn.k_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.2.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.2.self_attn.v_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.2.self_attn.v_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.2.self_attn.v_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.2.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.2.self_attn.o_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.2.self_attn.o_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.2.self_attn.o_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.2.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.2.mlp.gate_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.2.mlp.gate_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.2.mlp.gate_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.2.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.2.mlp.up_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.2.mlp.up_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.2.mlp.up_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.2.mlp.up_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.2.mlp.down_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.2.mlp.down_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.2.mlp.down_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.2.mlp.down_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.3.self_attn.q_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.3.self_attn.q_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.3.self_attn.q_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.3.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.3.self_attn.k_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.3.self_attn.k_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.3.self_attn.k_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.3.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.3.self_attn.v_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.3.self_attn.v_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.3.self_attn.v_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.3.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.3.self_attn.o_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.3.self_attn.o_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.3.self_attn.o_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.3.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.3.mlp.gate_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.3.mlp.gate_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.3.mlp.gate_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.3.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.3.mlp.up_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.3.mlp.up_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.3.mlp.up_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.3.mlp.up_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.3.mlp.down_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.3.mlp.down_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.3.mlp.down_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.3.mlp.down_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.4.self_attn.q_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.4.self_attn.q_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.4.self_attn.q_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.4.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.4.self_attn.k_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.4.self_attn.k_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.4.self_attn.k_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.4.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.4.self_attn.v_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.4.self_attn.v_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.4.self_attn.v_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.4.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.4.self_attn.o_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.4.self_attn.o_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.4.self_attn.o_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.4.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.4.mlp.gate_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.4.mlp.gate_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.4.mlp.gate_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.4.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.4.mlp.up_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.4.mlp.up_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.4.mlp.up_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.4.mlp.up_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.4.mlp.down_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.4.mlp.down_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.4.mlp.down_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.4.mlp.down_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.5.self_attn.q_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.5.self_attn.q_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.5.self_attn.q_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.5.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.5.self_attn.k_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.5.self_attn.k_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.5.self_attn.k_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.5.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.5.self_attn.v_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.5.self_attn.v_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.5.self_attn.v_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.5.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.5.self_attn.o_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.5.self_attn.o_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.5.self_attn.o_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.5.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.5.mlp.gate_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.5.mlp.gate_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.5.mlp.gate_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.5.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.5.mlp.up_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.5.mlp.up_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.5.mlp.up_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.5.mlp.up_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.5.mlp.down_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.5.mlp.down_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.5.mlp.down_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.5.mlp.down_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.6.self_attn.q_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.6.self_attn.q_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.6.self_attn.q_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.6.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.6.self_attn.k_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.6.self_attn.k_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.6.self_attn.k_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.6.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.6.self_attn.v_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.6.self_attn.v_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.6.self_attn.v_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.6.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.6.self_attn.o_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.6.self_attn.o_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.6.self_attn.o_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.6.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.6.mlp.gate_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.6.mlp.gate_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.6.mlp.gate_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.6.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.6.mlp.up_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.6.mlp.up_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.6.mlp.up_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.6.mlp.up_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.6.mlp.down_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.6.mlp.down_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.6.mlp.down_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.6.mlp.down_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.7.self_attn.q_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.7.self_attn.q_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.7.self_attn.q_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.7.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.7.self_attn.k_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.7.self_attn.k_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.7.self_attn.k_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.7.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.7.self_attn.v_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.7.self_attn.v_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.7.self_attn.v_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.7.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.7.self_attn.o_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.7.self_attn.o_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.7.self_attn.o_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.7.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.7.mlp.gate_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.7.mlp.gate_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.7.mlp.gate_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.7.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.7.mlp.up_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.7.mlp.up_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.7.mlp.up_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.7.mlp.up_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.7.mlp.down_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.7.mlp.down_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.7.mlp.down_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.7.mlp.down_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.8.self_attn.q_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.8.self_attn.q_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.8.self_attn.q_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.8.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.8.self_attn.k_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.8.self_attn.k_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.8.self_attn.k_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.8.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.8.self_attn.v_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.8.self_attn.v_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.8.self_attn.v_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.8.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.8.self_attn.o_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.8.self_attn.o_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.8.self_attn.o_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.8.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.8.mlp.gate_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.8.mlp.gate_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.8.mlp.gate_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.8.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.8.mlp.up_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.8.mlp.up_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.8.mlp.up_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.8.mlp.up_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.8.mlp.down_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.8.mlp.down_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.8.mlp.down_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.8.mlp.down_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.9.self_attn.q_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.9.self_attn.q_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.9.self_attn.q_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.9.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.9.self_attn.k_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.9.self_attn.k_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.9.self_attn.k_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.9.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.9.self_attn.v_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.9.self_attn.v_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.9.self_attn.v_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.9.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.9.self_attn.o_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.9.self_attn.o_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.9.self_attn.o_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.9.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.9.mlp.gate_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.9.mlp.gate_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.9.mlp.gate_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.9.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.9.mlp.up_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.9.mlp.up_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.9.mlp.up_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.9.mlp.up_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.9.mlp.down_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.9.mlp.down_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.9.mlp.down_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.9.mlp.down_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.10.self_attn.q_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.10.self_attn.q_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.10.self_attn.q_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.10.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.10.self_attn.k_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.10.self_attn.k_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.10.self_attn.k_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.10.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.10.self_attn.v_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.10.self_attn.v_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.10.self_attn.v_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.10.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.10.self_attn.o_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.10.self_attn.o_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.10.self_attn.o_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.10.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.10.mlp.gate_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.10.mlp.gate_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.10.mlp.gate_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.10.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.10.mlp.up_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.10.mlp.up_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.10.mlp.up_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.10.mlp.up_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.10.mlp.down_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.10.mlp.down_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.10.mlp.down_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.10.mlp.down_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.11.self_attn.q_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.11.self_attn.q_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.11.self_attn.q_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.11.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.11.self_attn.k_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.11.self_attn.k_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.11.self_attn.k_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.11.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.11.self_attn.v_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.11.self_attn.v_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.11.self_attn.v_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.11.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.11.self_attn.o_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.11.self_attn.o_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.11.self_attn.o_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.11.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.11.mlp.gate_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.11.mlp.gate_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.11.mlp.gate_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.11.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.11.mlp.up_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.11.mlp.up_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.11.mlp.up_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.11.mlp.up_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.11.mlp.down_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.11.mlp.down_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.11.mlp.down_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.11.mlp.down_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.12.self_attn.q_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.12.self_attn.q_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.12.self_attn.q_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.12.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.12.self_attn.k_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.12.self_attn.k_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.12.self_attn.k_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.12.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.12.self_attn.v_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.12.self_attn.v_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.12.self_attn.v_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.12.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.12.self_attn.o_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.12.self_attn.o_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.12.self_attn.o_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.12.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.12.mlp.gate_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.12.mlp.gate_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.12.mlp.gate_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.12.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.12.mlp.up_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.12.mlp.up_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.12.mlp.up_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.12.mlp.up_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.12.mlp.down_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.12.mlp.down_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.12.mlp.down_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.12.mlp.down_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.13.self_attn.q_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.13.self_attn.q_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.13.self_attn.q_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.13.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.13.self_attn.k_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.13.self_attn.k_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.13.self_attn.k_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.13.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.13.self_attn.v_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.13.self_attn.v_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.13.self_attn.v_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.13.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.13.self_attn.o_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.13.self_attn.o_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.13.self_attn.o_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.13.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.13.mlp.gate_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.13.mlp.gate_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.13.mlp.gate_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.13.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.13.mlp.up_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.13.mlp.up_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.13.mlp.up_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.13.mlp.up_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.13.mlp.down_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.13.mlp.down_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.13.mlp.down_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.13.mlp.down_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.14.self_attn.q_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.14.self_attn.q_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.14.self_attn.q_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.14.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.14.self_attn.k_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.14.self_attn.k_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.14.self_attn.k_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.14.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.14.self_attn.v_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.14.self_attn.v_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.14.self_attn.v_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.14.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.14.self_attn.o_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.14.self_attn.o_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.14.self_attn.o_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.14.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.14.mlp.gate_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.14.mlp.gate_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.14.mlp.gate_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.14.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.14.mlp.up_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.14.mlp.up_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.14.mlp.up_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.14.mlp.up_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.14.mlp.down_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.14.mlp.down_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.14.mlp.down_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.14.mlp.down_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.15.self_attn.q_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.15.self_attn.q_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.15.self_attn.q_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.15.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.15.self_attn.k_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.15.self_attn.k_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.15.self_attn.k_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.15.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.15.self_attn.v_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.15.self_attn.v_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.15.self_attn.v_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.15.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.15.self_attn.o_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.15.self_attn.o_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.15.self_attn.o_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.15.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.15.mlp.gate_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.15.mlp.gate_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.15.mlp.gate_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.15.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.15.mlp.up_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.15.mlp.up_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.15.mlp.up_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.15.mlp.up_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.15.mlp.down_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.15.mlp.down_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.15.mlp.down_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.15.mlp.down_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.lm_head.weight \t torch.Size([50304, 2048])\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/OLMo-1B-0724-hf\")\n",
    "# Use '[PAD]' or a similar valid token as the padding token\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "dataset = Dataset.from_pandas(forget_train_df)\n",
    "\n",
    "training_args = SFTConfig(\n",
    "    max_seq_length=512,\n",
    "    report_to='none',\n",
    "    output_dir=\"/tmp\",\n",
    "    dataset_text_field=\"text\",\n",
    "    packing=True,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "        model=olmo,\n",
    "        train_dataset=dataset,\n",
    "        args=training_args,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "olmo_finetuned = trainer.model\n",
    "analisis_modelo(olmo_finetuned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aplicacin del task Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaskVector():\n",
    "    def __init__(self, pretrained_model=None, finetuned_model=None, vector=None):\n",
    "        \"\"\"Initializes the task vector from a pretrained and a finetuned checkpoints.\n",
    "\n",
    "        This can either be done by passing two state dicts (one corresponding to the\n",
    "        pretrained model, and another to the finetuned model), or by directly passying in\n",
    "        the task vector state dict.\n",
    "        \"\"\"\n",
    "        if vector is not None:\n",
    "            self.vector = vector\n",
    "        else:\n",
    "            assert pretrained_model is not None and finetuned_model is not None\n",
    "            with torch.no_grad():\n",
    "                pretrained_state_dict = pretrained_model.base_model.state_dict()\n",
    "                finetuned_state_dict = finetuned_model.base_model.state_dict()\n",
    "                self.vector = {}\n",
    "                for key in pretrained_state_dict:\n",
    "                    if pretrained_state_dict[key].dtype in [torch.int64, torch.uint8]:\n",
    "                        continue\n",
    "                    self.vector[key] = finetuned_state_dict[key] - pretrained_state_dict[key]\n",
    "\n",
    "    def __add__(self, other):\n",
    "        \"\"\"Add two task vectors together.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            new_vector = {}\n",
    "            for key in self.vector:\n",
    "                if key not in other.vector:\n",
    "                    print(f'Warning, key {key} is not present in both task vectors.')\n",
    "                    continue\n",
    "                new_vector[key] = self.vector[key] + other.vector[key]\n",
    "        return TaskVector(vector=new_vector)\n",
    "\n",
    "    def __radd__(self, other):\n",
    "        if other is None or isinstance(other, int):\n",
    "            return self\n",
    "        return self.__add__(other)\n",
    "\n",
    "    def __neg__(self):\n",
    "        \"\"\"Negate a task vector.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            new_vector = {}\n",
    "            for key in self.vector:\n",
    "                new_vector[key] = - self.vector[key]\n",
    "        return TaskVector(vector=new_vector)\n",
    "\n",
    "    def apply_to(self, pre_model, scaling_coef=1.0):\n",
    "        \"\"\"Apply a task vector to a pretrained model.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            pretrained_model = pre_model\n",
    "            new_state_dict = {}\n",
    "            pretrained_state_dict = pretrained_model.base_model.state_dict()\n",
    "            for key in pretrained_state_dict:\n",
    "                if key not in self.vector:\n",
    "                    print(f'Warning: key {key} is present in the pretrained state dict but not in the task vector')\n",
    "                    continue\n",
    "                new_state_dict[key] = pretrained_state_dict[key] + scaling_coef * self.vector[key]\n",
    "        pretrained_model.base_model.load_state_dict(new_state_dict, strict=False)\n",
    "        return pretrained_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.TaskVector object at 0x746c1bd10370>\n",
      "Warning: key model.model.layers.0.self_attn.q_proj.base_layer.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.0.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.0.self_attn.k_proj.base_layer.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.0.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.0.self_attn.v_proj.base_layer.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.0.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.0.self_attn.o_proj.base_layer.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.0.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.0.mlp.gate_proj.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.0.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.0.mlp.up_proj.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.0.mlp.up_proj.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.0.mlp.down_proj.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.0.mlp.down_proj.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.1.self_attn.q_proj.base_layer.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.1.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.1.self_attn.k_proj.base_layer.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.1.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.1.self_attn.v_proj.base_layer.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.1.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.1.self_attn.o_proj.base_layer.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.1.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.1.mlp.gate_proj.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.1.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.1.mlp.up_proj.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.1.mlp.up_proj.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.1.mlp.down_proj.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.1.mlp.down_proj.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.2.self_attn.q_proj.base_layer.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.2.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.2.self_attn.k_proj.base_layer.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.2.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.2.self_attn.v_proj.base_layer.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.2.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.2.self_attn.o_proj.base_layer.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.2.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.2.mlp.gate_proj.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.2.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.2.mlp.up_proj.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.2.mlp.up_proj.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.2.mlp.down_proj.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.2.mlp.down_proj.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.3.self_attn.q_proj.base_layer.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.3.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.3.self_attn.k_proj.base_layer.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.3.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.3.self_attn.v_proj.base_layer.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.3.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.3.self_attn.o_proj.base_layer.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.3.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.3.mlp.gate_proj.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.3.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.3.mlp.up_proj.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.3.mlp.up_proj.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.3.mlp.down_proj.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.3.mlp.down_proj.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.4.self_attn.q_proj.base_layer.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.4.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.4.self_attn.k_proj.base_layer.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.4.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.4.self_attn.v_proj.base_layer.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.4.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.4.self_attn.o_proj.base_layer.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.4.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.4.mlp.gate_proj.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.4.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.4.mlp.up_proj.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.4.mlp.up_proj.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.4.mlp.down_proj.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.4.mlp.down_proj.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.5.self_attn.q_proj.base_layer.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.5.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.5.self_attn.k_proj.base_layer.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.5.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.5.self_attn.v_proj.base_layer.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.5.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.5.self_attn.o_proj.base_layer.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.5.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.5.mlp.gate_proj.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.5.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.5.mlp.up_proj.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.5.mlp.up_proj.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.5.mlp.down_proj.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.5.mlp.down_proj.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.6.self_attn.q_proj.base_layer.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.6.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.6.self_attn.k_proj.base_layer.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.6.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.6.self_attn.v_proj.base_layer.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.6.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.6.self_attn.o_proj.base_layer.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.6.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.6.mlp.gate_proj.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.6.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.6.mlp.up_proj.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.6.mlp.up_proj.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.6.mlp.down_proj.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.6.mlp.down_proj.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.7.self_attn.q_proj.base_layer.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.7.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.7.self_attn.k_proj.base_layer.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.7.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.7.self_attn.v_proj.base_layer.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.7.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.7.self_attn.o_proj.base_layer.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.7.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.7.mlp.gate_proj.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.7.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.7.mlp.up_proj.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.7.mlp.up_proj.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.7.mlp.down_proj.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.7.mlp.down_proj.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.8.self_attn.q_proj.base_layer.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.8.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.8.self_attn.k_proj.base_layer.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.8.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.8.self_attn.v_proj.base_layer.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.8.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.8.self_attn.o_proj.base_layer.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.8.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.8.mlp.gate_proj.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.8.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.8.mlp.up_proj.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.8.mlp.up_proj.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.8.mlp.down_proj.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.8.mlp.down_proj.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.9.self_attn.q_proj.base_layer.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.9.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.9.self_attn.k_proj.base_layer.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.9.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.9.self_attn.v_proj.base_layer.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.9.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.9.self_attn.o_proj.base_layer.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.9.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.9.mlp.gate_proj.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.9.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.9.mlp.up_proj.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.9.mlp.up_proj.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.9.mlp.down_proj.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.9.mlp.down_proj.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.10.self_attn.q_proj.base_layer.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.10.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.10.self_attn.k_proj.base_layer.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.10.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.10.self_attn.v_proj.base_layer.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.10.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.10.self_attn.o_proj.base_layer.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.10.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.10.mlp.gate_proj.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.10.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.10.mlp.up_proj.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.10.mlp.up_proj.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.10.mlp.down_proj.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.10.mlp.down_proj.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.11.self_attn.q_proj.base_layer.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.11.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.11.self_attn.k_proj.base_layer.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.11.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.11.self_attn.v_proj.base_layer.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.11.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.11.self_attn.o_proj.base_layer.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.11.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.11.mlp.gate_proj.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.11.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.11.mlp.up_proj.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.11.mlp.up_proj.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.11.mlp.down_proj.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.11.mlp.down_proj.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.12.self_attn.q_proj.base_layer.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.12.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.12.self_attn.k_proj.base_layer.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.12.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.12.self_attn.v_proj.base_layer.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.12.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.12.self_attn.o_proj.base_layer.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.12.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.12.mlp.gate_proj.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.12.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.12.mlp.up_proj.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.12.mlp.up_proj.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.12.mlp.down_proj.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.12.mlp.down_proj.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.13.self_attn.q_proj.base_layer.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.13.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.13.self_attn.k_proj.base_layer.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.13.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.13.self_attn.v_proj.base_layer.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.13.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.13.self_attn.o_proj.base_layer.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.13.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.13.mlp.gate_proj.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.13.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.13.mlp.up_proj.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.13.mlp.up_proj.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.13.mlp.down_proj.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.13.mlp.down_proj.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.14.self_attn.q_proj.base_layer.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.14.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.14.self_attn.k_proj.base_layer.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.14.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.14.self_attn.v_proj.base_layer.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.14.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.14.self_attn.o_proj.base_layer.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.14.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.14.mlp.gate_proj.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.14.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.14.mlp.up_proj.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.14.mlp.up_proj.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.14.mlp.down_proj.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.14.mlp.down_proj.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.15.self_attn.q_proj.base_layer.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.15.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.15.self_attn.k_proj.base_layer.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.15.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.15.self_attn.v_proj.base_layer.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.15.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.15.self_attn.o_proj.base_layer.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.15.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.15.mlp.gate_proj.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.15.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.15.mlp.up_proj.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.15.mlp.up_proj.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.15.mlp.down_proj.weight is present in the pretrained state dict but not in the task vector\n",
      "Warning: key model.model.layers.15.mlp.down_proj.weight.quant_state.bitsandbytes__nf4 is present in the pretrained state dict but not in the task vector\n",
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): OlmoForCausalLM(\n",
      "      (model): OlmoModel(\n",
      "        (embed_tokens): Embedding(50304, 2048, padding_idx=1)\n",
      "        (layers): ModuleList(\n",
      "          (0-15): 16 x OlmoDecoderLayer(\n",
      "            (self_attn): OlmoAttention(\n",
      "              (q_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (o_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (mlp): OlmoMLP(\n",
      "              (gate_proj): Linear4bit(in_features=2048, out_features=8192, bias=False)\n",
      "              (up_proj): Linear4bit(in_features=2048, out_features=8192, bias=False)\n",
      "              (down_proj): Linear4bit(in_features=8192, out_features=2048, bias=False)\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): OlmoLayerNorm()\n",
      "            (post_attention_layernorm): OlmoLayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (norm): OlmoLayerNorm()\n",
      "        (rotary_emb): OlmoRotaryEmbedding()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=2048, out_features=50304, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "base_model.model.model.embed_tokens.weight \t torch.Size([50304, 2048])\n",
      "base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.0.mlp.gate_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.0.mlp.gate_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.0.mlp.gate_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.0.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.0.mlp.up_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.0.mlp.up_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.0.mlp.up_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.0.mlp.up_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.0.mlp.down_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.0.mlp.down_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.0.mlp.down_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.0.mlp.down_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.1.self_attn.k_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.1.self_attn.k_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.1.self_attn.k_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.1.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.1.self_attn.v_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.1.self_attn.v_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.1.self_attn.v_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.1.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.1.self_attn.o_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.1.self_attn.o_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.1.self_attn.o_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.1.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.1.mlp.gate_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.1.mlp.gate_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.1.mlp.gate_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.1.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.1.mlp.up_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.1.mlp.up_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.1.mlp.up_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.1.mlp.up_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.1.mlp.down_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.1.mlp.down_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.1.mlp.down_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.1.mlp.down_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.2.self_attn.q_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.2.self_attn.q_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.2.self_attn.q_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.2.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.2.self_attn.k_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.2.self_attn.k_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.2.self_attn.k_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.2.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.2.self_attn.v_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.2.self_attn.v_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.2.self_attn.v_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.2.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.2.self_attn.o_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.2.self_attn.o_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.2.self_attn.o_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.2.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.2.mlp.gate_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.2.mlp.gate_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.2.mlp.gate_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.2.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.2.mlp.up_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.2.mlp.up_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.2.mlp.up_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.2.mlp.up_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.2.mlp.down_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.2.mlp.down_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.2.mlp.down_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.2.mlp.down_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.3.self_attn.q_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.3.self_attn.q_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.3.self_attn.q_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.3.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.3.self_attn.k_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.3.self_attn.k_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.3.self_attn.k_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.3.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.3.self_attn.v_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.3.self_attn.v_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.3.self_attn.v_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.3.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.3.self_attn.o_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.3.self_attn.o_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.3.self_attn.o_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.3.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.3.mlp.gate_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.3.mlp.gate_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.3.mlp.gate_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.3.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.3.mlp.up_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.3.mlp.up_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.3.mlp.up_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.3.mlp.up_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.3.mlp.down_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.3.mlp.down_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.3.mlp.down_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.3.mlp.down_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.4.self_attn.q_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.4.self_attn.q_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.4.self_attn.q_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.4.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.4.self_attn.k_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.4.self_attn.k_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.4.self_attn.k_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.4.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.4.self_attn.v_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.4.self_attn.v_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.4.self_attn.v_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.4.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.4.self_attn.o_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.4.self_attn.o_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.4.self_attn.o_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.4.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.4.mlp.gate_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.4.mlp.gate_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.4.mlp.gate_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.4.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.4.mlp.up_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.4.mlp.up_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.4.mlp.up_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.4.mlp.up_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.4.mlp.down_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.4.mlp.down_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.4.mlp.down_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.4.mlp.down_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.5.self_attn.q_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.5.self_attn.q_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.5.self_attn.q_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.5.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.5.self_attn.k_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.5.self_attn.k_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.5.self_attn.k_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.5.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.5.self_attn.v_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.5.self_attn.v_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.5.self_attn.v_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.5.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.5.self_attn.o_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.5.self_attn.o_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.5.self_attn.o_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.5.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.5.mlp.gate_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.5.mlp.gate_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.5.mlp.gate_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.5.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.5.mlp.up_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.5.mlp.up_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.5.mlp.up_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.5.mlp.up_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.5.mlp.down_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.5.mlp.down_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.5.mlp.down_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.5.mlp.down_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.6.self_attn.q_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.6.self_attn.q_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.6.self_attn.q_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.6.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.6.self_attn.k_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.6.self_attn.k_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.6.self_attn.k_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.6.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.6.self_attn.v_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.6.self_attn.v_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.6.self_attn.v_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.6.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.6.self_attn.o_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.6.self_attn.o_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.6.self_attn.o_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.6.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.6.mlp.gate_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.6.mlp.gate_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.6.mlp.gate_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.6.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.6.mlp.up_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.6.mlp.up_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.6.mlp.up_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.6.mlp.up_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.6.mlp.down_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.6.mlp.down_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.6.mlp.down_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.6.mlp.down_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.7.self_attn.q_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.7.self_attn.q_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.7.self_attn.q_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.7.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.7.self_attn.k_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.7.self_attn.k_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.7.self_attn.k_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.7.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.7.self_attn.v_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.7.self_attn.v_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.7.self_attn.v_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.7.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.7.self_attn.o_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.7.self_attn.o_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.7.self_attn.o_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.7.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.7.mlp.gate_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.7.mlp.gate_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.7.mlp.gate_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.7.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.7.mlp.up_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.7.mlp.up_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.7.mlp.up_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.7.mlp.up_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.7.mlp.down_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.7.mlp.down_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.7.mlp.down_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.7.mlp.down_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.8.self_attn.q_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.8.self_attn.q_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.8.self_attn.q_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.8.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.8.self_attn.k_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.8.self_attn.k_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.8.self_attn.k_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.8.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.8.self_attn.v_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.8.self_attn.v_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.8.self_attn.v_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.8.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.8.self_attn.o_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.8.self_attn.o_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.8.self_attn.o_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.8.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.8.mlp.gate_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.8.mlp.gate_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.8.mlp.gate_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.8.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.8.mlp.up_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.8.mlp.up_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.8.mlp.up_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.8.mlp.up_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.8.mlp.down_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.8.mlp.down_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.8.mlp.down_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.8.mlp.down_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.9.self_attn.q_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.9.self_attn.q_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.9.self_attn.q_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.9.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.9.self_attn.k_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.9.self_attn.k_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.9.self_attn.k_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.9.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.9.self_attn.v_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.9.self_attn.v_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.9.self_attn.v_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.9.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.9.self_attn.o_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.9.self_attn.o_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.9.self_attn.o_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.9.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.9.mlp.gate_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.9.mlp.gate_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.9.mlp.gate_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.9.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.9.mlp.up_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.9.mlp.up_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.9.mlp.up_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.9.mlp.up_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.9.mlp.down_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.9.mlp.down_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.9.mlp.down_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.9.mlp.down_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.10.self_attn.q_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.10.self_attn.q_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.10.self_attn.q_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.10.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.10.self_attn.k_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.10.self_attn.k_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.10.self_attn.k_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.10.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.10.self_attn.v_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.10.self_attn.v_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.10.self_attn.v_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.10.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.10.self_attn.o_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.10.self_attn.o_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.10.self_attn.o_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.10.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.10.mlp.gate_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.10.mlp.gate_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.10.mlp.gate_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.10.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.10.mlp.up_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.10.mlp.up_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.10.mlp.up_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.10.mlp.up_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.10.mlp.down_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.10.mlp.down_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.10.mlp.down_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.10.mlp.down_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.11.self_attn.q_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.11.self_attn.q_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.11.self_attn.q_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.11.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.11.self_attn.k_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.11.self_attn.k_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.11.self_attn.k_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.11.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.11.self_attn.v_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.11.self_attn.v_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.11.self_attn.v_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.11.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.11.self_attn.o_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.11.self_attn.o_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.11.self_attn.o_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.11.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.11.mlp.gate_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.11.mlp.gate_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.11.mlp.gate_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.11.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.11.mlp.up_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.11.mlp.up_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.11.mlp.up_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.11.mlp.up_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.11.mlp.down_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.11.mlp.down_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.11.mlp.down_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.11.mlp.down_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.12.self_attn.q_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.12.self_attn.q_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.12.self_attn.q_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.12.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.12.self_attn.k_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.12.self_attn.k_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.12.self_attn.k_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.12.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.12.self_attn.v_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.12.self_attn.v_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.12.self_attn.v_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.12.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.12.self_attn.o_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.12.self_attn.o_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.12.self_attn.o_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.12.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.12.mlp.gate_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.12.mlp.gate_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.12.mlp.gate_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.12.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.12.mlp.up_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.12.mlp.up_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.12.mlp.up_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.12.mlp.up_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.12.mlp.down_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.12.mlp.down_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.12.mlp.down_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.12.mlp.down_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.13.self_attn.q_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.13.self_attn.q_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.13.self_attn.q_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.13.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.13.self_attn.k_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.13.self_attn.k_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.13.self_attn.k_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.13.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.13.self_attn.v_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.13.self_attn.v_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.13.self_attn.v_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.13.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.13.self_attn.o_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.13.self_attn.o_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.13.self_attn.o_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.13.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.13.mlp.gate_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.13.mlp.gate_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.13.mlp.gate_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.13.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.13.mlp.up_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.13.mlp.up_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.13.mlp.up_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.13.mlp.up_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.13.mlp.down_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.13.mlp.down_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.13.mlp.down_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.13.mlp.down_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.14.self_attn.q_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.14.self_attn.q_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.14.self_attn.q_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.14.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.14.self_attn.k_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.14.self_attn.k_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.14.self_attn.k_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.14.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.14.self_attn.v_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.14.self_attn.v_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.14.self_attn.v_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.14.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.14.self_attn.o_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.14.self_attn.o_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.14.self_attn.o_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.14.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.14.mlp.gate_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.14.mlp.gate_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.14.mlp.gate_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.14.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.14.mlp.up_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.14.mlp.up_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.14.mlp.up_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.14.mlp.up_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.14.mlp.down_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.14.mlp.down_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.14.mlp.down_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.14.mlp.down_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.15.self_attn.q_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.15.self_attn.q_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.15.self_attn.q_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.15.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.15.self_attn.k_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.15.self_attn.k_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.15.self_attn.k_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.15.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.15.self_attn.v_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.15.self_attn.v_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.15.self_attn.v_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.15.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.15.self_attn.o_proj.base_layer.weight \t torch.Size([2097152, 1])\n",
      "base_model.model.model.layers.15.self_attn.o_proj.base_layer.weight.absmax \t torch.Size([65536])\n",
      "base_model.model.model.layers.15.self_attn.o_proj.base_layer.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.15.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight \t torch.Size([8, 2048])\n",
      "base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight \t torch.Size([2048, 8])\n",
      "base_model.model.model.layers.15.mlp.gate_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.15.mlp.gate_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.15.mlp.gate_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.15.mlp.gate_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.15.mlp.up_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.15.mlp.up_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.15.mlp.up_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.15.mlp.up_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.model.layers.15.mlp.down_proj.weight \t torch.Size([8388608, 1])\n",
      "base_model.model.model.layers.15.mlp.down_proj.weight.absmax \t torch.Size([262144])\n",
      "base_model.model.model.layers.15.mlp.down_proj.weight.quant_map \t torch.Size([16])\n",
      "base_model.model.model.layers.15.mlp.down_proj.weight.quant_state.bitsandbytes__nf4 \t torch.Size([81])\n",
      "base_model.model.lm_head.weight \t torch.Size([50304, 2048])\n"
     ]
    }
   ],
   "source": [
    "# Create the task vector\n",
    "task_vector = TaskVector(olmo, olmo_finetuned)\n",
    "# Negate the task vector\n",
    "neg_task_vector = -task_vector\n",
    "print(neg_task_vector)\n",
    "# Apply the task vector\n",
    "result_model = neg_task_vector.apply_to(olmo, scaling_coef=0.5)\n",
    "\n",
    "analisis_modelo(result_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#result_model.save_pretrained(\"./models/result_model_TaskVector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/diegohernandez/.local/lib/python3.10/site-packages/peft/tuners/lora/bnb.py:355: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#result_model.base_model.save_pretrained(\"./models/test/result_model_TaskVector\")\n",
    "result_model = result_model.merge_and_unload()\n",
    "result_model.save_pretrained(\"./models/test0/result_model_TaskVector\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
